{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SurvivalLVQ Analysis: Veterans' Lung Cancer Dataset\n",
    "\n",
    "This notebook demonstrates the power of the **SurvivalLVQ** algorithm on the Veterans' Administration Lung Cancer Trial dataset. The dataset contains information about male patients with advanced inoperable lung cancer, comparing standard and test chemotherapy treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sksurv.util (from versions: none)\n",
      "ERROR: No matching distribution found for sksurv.util\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sksurv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSurvivalLVQ\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SurvivalLVQ\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSkewTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SkewTransformer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "File \u001b[1;32mc:\\Users\\votev\\OneDrive - KU Leuven\\Git\\SurvivalLVQ\\Models\\SurvivalLVQ.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msksurv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Surv\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     10\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sksurv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Models.SurvivalLVQ import SurvivalLVQ\n",
    "from SkewTransformer import SkewTransformer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.util import Surv\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sksurv.compare import compare_survival\n",
    "from utils import score_brier, score_CI, score_CI_ipcw\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "\n",
    "Let's first explore the Veterans' lung cancer dataset to understand its structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Veterans dataset\n",
    "print(\"=== Veterans' Lung Cancer Dataset Exploration ===\")\n",
    "print()\n",
    "\n",
    "data, fac_col_ids, num_col_ids, T, D = load_dataset('veteran')\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Number of samples: {data.shape[0]}\")\n",
    "print(f\"Number of features: {data.shape[1]}\")\n",
    "print(f\"Categorical feature indices: {fac_col_ids}\")\n",
    "print(f\"Numerical feature indices: {num_col_ids}\")\n",
    "\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    feature_type = \"categorical\" if i in fac_col_ids else \"numerical\"\n",
    "    print(f\"  {i}: {col} ({feature_type})\")\n",
    "\n",
    "print(f\"\\nSurvival time statistics (days):\")\n",
    "print(f\"  Min time: {T.min():.2f}\")\n",
    "print(f\"  Max time: {T.max():.2f}\")\n",
    "print(f\"  Mean time: {T.mean():.2f}\")\n",
    "print(f\"  Median time: {T.median():.2f}\")\n",
    "\n",
    "print(f\"\\nCensoring information:\")\n",
    "print(f\"  Number of events (deaths): {D.sum()}\")\n",
    "print(f\"  Number of censored: {(~D).sum()}\")\n",
    "print(f\"  Censoring rate: {(~D).mean():.3f}\")\n",
    "\n",
    "print(f\"\\nMissing values per feature:\")\n",
    "for col in data.columns:\n",
    "    missing_count = data[col].isna().sum()\n",
    "    missing_pct = (missing_count / len(data)) * 100\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "if data.isna().sum().sum() == 0:\n",
    "    print(\"  No missing values!\")\n",
    "\n",
    "print(f\"\\nFirst few rows of the dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We prepare the data by:\n",
    "1. Converting to numpy arrays\n",
    "2. Creating survival objects from censoring indicators and survival times\n",
    "3. Splitting into training (80%) and test (20%) sets, stratified by censoring status\n",
    "4. Imputing any missing values with median\n",
    "5. Applying z-transformation and skew correction to numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = data.astype(float).to_numpy()                  # covariates\n",
    "D = D.to_numpy()                                   # censoring indicator\n",
    "T = T.to_numpy()                                   # survival time\n",
    "Y = Surv().from_arrays(D.astype('?'), T)           # convert (D, T) to survival object\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, stratify=D, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Impute missing values with median\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "# Transform numerical features to have a normal distribution\n",
    "if len(num_col_ids) > 0:\n",
    "    scaler = SkewTransformer()\n",
    "    scaler.fit(X_train[:, num_col_ids])\n",
    "    X_train[:, num_col_ids] = scaler.transform(X_train[:, num_col_ids])\n",
    "    X_test[:, num_col_ids] = scaler.transform(X_test[:, num_col_ids])\n",
    "    print(f\"\\nNumerical features ({len(num_col_ids)}) transformed with SkewTransformer\")\n",
    "\n",
    "print(\"\\nData preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SurvivalLVQ Model\n",
    "\n",
    "Now we train the SurvivalLVQ model. The algorithm learns **prototype vectors** that represent different risk groups in the data. These prototypes are interpretable and can reveal patient subgroups with distinct survival patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = SurvivalLVQ(\n",
    "    n_prototypes=3,           # Learn 3 risk groups\n",
    "    batch_size=64,            # Smaller batch size for this dataset\n",
    "    lr=7e-3,                  # Learning rate\n",
    "    epochs=50,                # Training epochs\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training SurvivalLVQ model...\\n\")\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "We evaluate the model using three key metrics:\n",
    "\n",
    "1. **Concordance Index (C-Index)**: Measures the model's ability to correctly rank patients by risk\n",
    "2. **IPCW C-Index**: C-Index adjusted for censoring using Inverse Probability of Censoring Weighting\n",
    "3. **Integrated Brier Score**: Measures prediction accuracy at different time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"=== Model Performance on Test Set ===\")\n",
    "print()\n",
    "\n",
    "ci_score = score_CI(model, X_test, Y_test)\n",
    "print(f\"C-Index (Concordance Index): {ci_score:.4f}\")\n",
    "\n",
    "ci_ipcw_score = score_CI_ipcw(model, X_test, Y_train, Y_test)\n",
    "print(f\"IPCW C-Index: {ci_ipcw_score:.4f}\")\n",
    "\n",
    "brier_score = score_brier(model, X_test, Y_train, Y_test)\n",
    "print(f\"Integrated Brier Score: {brier_score:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Higher C-Index values (closer to 1.0) indicate better risk stratification\")\n",
    "print(\"✓ Lower Brier scores indicate better prediction accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Risk Groups\n",
    "\n",
    "The **key power of SurvivalLVQ** lies in its interpretability. The model learns prototype vectors that define distinct risk groups. Let's visualize the survival curves for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned risk groups\n",
    "D_train, T_train = map(np.array, zip(*Y_train))\n",
    "model.vis(X_train, D_train, T_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing\n",
    "\n",
    "We use the **log-rank test** to determine if the survival differences between the learned risk groups are statistically significant. A low p-value (< 0.05) indicates that the groups have significantly different survival patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign patients to their closest prototype (risk group)\n",
    "group = model.predict(X_train, closest=True)\n",
    "\n",
    "# Perform log-rank test\n",
    "test_statistic, p_value = compare_survival(Y_train, group)\n",
    "\n",
    "print(\"=== Log-Rank Test for Risk Group Separation ===\")\n",
    "print()\n",
    "print(f\"Test statistic: {test_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")\n",
    "print()\n",
    "\n",
    "if p_value < 0.001:\n",
    "    print(\"✓ HIGHLY SIGNIFICANT (p < 0.001)\")\n",
    "    print(\"  The risk groups have dramatically different survival patterns!\")\n",
    "elif p_value < 0.05:\n",
    "    print(\"✓ SIGNIFICANT (p < 0.05)\")\n",
    "    print(\"  The risk groups have significantly different survival patterns.\")\n",
    "else:\n",
    "    print(\"✗ NOT SIGNIFICANT (p >= 0.05)\")\n",
    "    print(\"  The risk groups do not show statistically significant differences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Analyzing Prototype Characteristics\n\nOne of the **most powerful features** of SurvivalLVQ is that the learned prototypes are **interpretable**. We can examine the prototype vectors to understand what characterizes each risk group.\n\n### Understanding the Three Risk Groups:\n\nThe model identified **3 distinct patient risk profiles**:\n\n**Prototype 0 (22.0% of patients) - Youngest, Small Cell Carcinoma:**\n- **Strongest characteristic**: Young age (num_age = -0.84, well below average)\n- **Cell type**: Predominantly small cell carcinoma (0.94, very strong indicator)\n- **Karno score**: Slightly above average performance status (0.15)\n- **Treatment**: Standard treatment (fac_trt_2 near 0)\n- This group represents younger patients with small cell lung cancer and relatively good functional status\n\n**Prototype 1 (48.6% of patients) - Best Performance Status:**\n- **Strongest characteristic**: Excellent Karnofsky score (0.96) indicating best functional status\n- **Cell type**: Mixed, with strong tendency toward large cell (0.61)\n- **Prior therapy**: Moderate prior treatment (0.38)\n- **Age**: Near average age (-0.03)\n- **Treatment**: Some test treatment (0.38)\n- This is the largest group, representing patients in the best physical condition regardless of age\n\n**Prototype 2 (29.4% of patients) - Worst Prognosis Profile:**\n- **Strongest characteristic**: Very poor Karnofsky score (-1.40) indicating severely impaired functional status\n- **Cell type**: Strong adenocarcinoma tendency (0.69)\n- **Treatment**: More likely test treatment (0.76)\n- **Prior therapy**: High likelihood of prior treatment (0.66)\n- **Age**: Slightly older than average (0.09)\n- This group represents the highest-risk patients with poor functional status and more aggressive disease\n\n### Key Clinical Insights:\n\n**Most Important Risk Factor**: The **Karnofsky performance score** shows the widest spread across prototypes (range: 2.36 from -1.40 to +0.96), making it the strongest discriminator between risk groups.\n\n**Age Pattern**: Age shows interesting variation - Prototype 0 is notably younger (-0.84), while Prototypes 1 and 2 are near average, suggesting age is secondary to functional status for risk stratification.\n\n**Cell Type Patterns**:\n- Prototype 0: Small cell carcinoma dominant\n- Prototype 1: Large cell carcinoma tendency  \n- Prototype 2: Adenocarcinoma dominant\n\n**Treatment Distribution**: Higher-risk patients (Prototype 2) were more likely to receive the test treatment, possibly indicating treatment assignment based on disease severity.\n\n**Clinical Takeaway**: The model automatically discovered that **functional status (Karnofsky score)** is the primary driver of risk stratification, with **age**, **cell type**, and **prior treatment history** as secondary factors. This aligns with clinical knowledge that performance status is a critical prognostic indicator in cancer patients.\n\n### Understanding the Values:\n- **Prototype values are standardized** (z-scores after skew transformation for numerical features)\n- **Negative values** indicate below-average, **positive values** indicate above-average\n- **Larger absolute values** indicate stronger deviation from the population mean\n- The model automatically discovered clinically meaningful patient subgroups without being explicitly told about these relationships!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get the learned prototypes\nprototypes = model.w.detach().cpu().numpy()\n\nprint(\"=== Learned Prototype Characteristics ===\")\nprint()\nprint(f\"Number of prototypes (risk groups): {prototypes.shape[0]}\")\nprint(f\"Feature dimension: {prototypes.shape[1]}\")\nprint()\n\n# Display prototype values for each feature\nprint(\"Prototype values by feature:\")\nprint()\n\nfor i, col in enumerate(data.columns):\n    feature_type = \"categorical\" if i in fac_col_ids else \"numerical\"\n    print(f\"{col} ({feature_type}):\")\n    for p_idx in range(prototypes.shape[0]):\n        print(f\"  Prototype {p_idx}: {prototypes[p_idx, i]:.4f}\")\n    print()\n\n# Calculate and display patients per group\nprint(\"\\n=== Patient Distribution Across Risk Groups ===\")\nprint()\nunique, counts = np.unique(group, return_counts=True)\nfor g, count in zip(unique, counts):\n    percentage = (count / len(group)) * 100\n    print(f\"Group {g}: {count} patients ({percentage:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Power of SurvivalLVQ\n",
    "\n",
    "This notebook demonstrated several key strengths of the SurvivalLVQ algorithm:\n",
    "\n",
    "### 1. **Interpretability**\n",
    "- Learns a small number of prototype vectors representing distinct risk groups\n",
    "- Prototypes can be examined to understand what characterizes high vs. low risk patients\n",
    "- Each patient is assigned to their closest prototype, providing clear risk stratification\n",
    "\n",
    "### 2. **Strong Predictive Performance**\n",
    "- Achieves high concordance index for ranking patients by risk\n",
    "- Provides accurate survival predictions across different time horizons\n",
    "\n",
    "### 3. **Statistical Validation**\n",
    "- Risk groups show statistically significant separation (log-rank test)\n",
    "- Demonstrates that the learned groups represent real differences in survival patterns\n",
    "\n",
    "### 4. **Handles Complex Data**\n",
    "- Works with both categorical and numerical features\n",
    "- Automatically handles right-censored survival data\n",
    "- Robust to missing values through preprocessing\n",
    "\n",
    "### 5. **Clinical Applicability**\n",
    "- Provides actionable risk stratification for patient management\n",
    "- Visualizations clearly show survival differences between groups\n",
    "- Can guide treatment decisions and resource allocation\n",
    "\n",
    "**SurvivalLVQ bridges the gap between black-box prediction models and interpretable clinical tools**, making it valuable for both research and clinical practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}